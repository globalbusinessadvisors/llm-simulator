---
# ==============================================================================
# Horizontal Pod Autoscaler - Standard
# ==============================================================================
# Auto-scales based on CPU and memory utilization
# ==============================================================================
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: llm-simulator-hpa
  namespace: llm-devops
  labels:
    app: llm-simulator
    component: autoscaling
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: llm-simulator

  minReplicas: 3
  maxReplicas: 20

  metrics:
    # CPU-based scaling
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 70

    # Memory-based scaling
    - type: Resource
      resource:
        name: memory
        target:
          type: Utilization
          averageUtilization: 80

  behavior:
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
        - type: Percent
          value: 50
          periodSeconds: 60
        - type: Pods
          value: 2
          periodSeconds: 60
      selectPolicy: Min

    scaleUp:
      stabilizationWindowSeconds: 60
      policies:
        - type: Percent
          value: 100
          periodSeconds: 30
        - type: Pods
          value: 4
          periodSeconds: 30
      selectPolicy: Max

---
# ==============================================================================
# Horizontal Pod Autoscaler - Custom Metrics
# ==============================================================================
# Advanced auto-scaling based on custom application metrics
# Requires metrics-server and custom metrics API
# ==============================================================================
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: llm-simulator-hpa-custom
  namespace: llm-devops
  labels:
    app: llm-simulator
    component: autoscaling
    type: custom-metrics
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: llm-simulator

  minReplicas: 3
  maxReplicas: 50

  metrics:
    # CPU utilization
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 70

    # Memory utilization
    - type: Resource
      resource:
        name: memory
        target:
          type: Utilization
          averageUtilization: 80

    # Custom metric: Request rate (requests per second)
    - type: Pods
      pods:
        metric:
          name: llm_simulator_requests_per_second
        target:
          type: AverageValue
          averageValue: "100"

    # Custom metric: Request queue depth
    - type: Pods
      pods:
        metric:
          name: llm_simulator_queue_depth
        target:
          type: AverageValue
          averageValue: "50"

    # Custom metric: P95 latency (milliseconds)
    - type: Pods
      pods:
        metric:
          name: llm_simulator_latency_p95_ms
        target:
          type: AverageValue
          averageValue: "1000"

    # External metric: Load balancer request rate
    - type: External
      external:
        metric:
          name: loadbalancer_requests_per_second
          selector:
            matchLabels:
              service: llm-simulator
        target:
          type: AverageValue
          averageValue: "500"

  behavior:
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
        - type: Percent
          value: 30
          periodSeconds: 120
        - type: Pods
          value: 2
          periodSeconds: 120
      selectPolicy: Min

    scaleUp:
      stabilizationWindowSeconds: 30
      policies:
        - type: Percent
          value: 100
          periodSeconds: 15
        - type: Pods
          value: 8
          periodSeconds: 15
      selectPolicy: Max

---
# ==============================================================================
# Vertical Pod Autoscaler (VPA)
# ==============================================================================
# Automatically adjusts resource requests/limits
# Note: VPA requires the VPA admission controller to be installed
# ==============================================================================
apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: llm-simulator-vpa
  namespace: llm-devops
  labels:
    app: llm-simulator
    component: autoscaling
spec:
  targetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: llm-simulator

  updatePolicy:
    updateMode: "Auto"  # Off, Initial, Recreate, Auto

  resourcePolicy:
    containerPolicies:
      - containerName: llm-simulator
        minAllowed:
          cpu: 500m
          memory: 1Gi
        maxAllowed:
          cpu: 8000m
          memory: 16Gi
        controlledResources:
          - cpu
          - memory
        mode: Auto

---
# ==============================================================================
# KEDA ScaledObject - Event-Driven Autoscaling
# ==============================================================================
# Scales to zero and event-driven scaling using KEDA
# Requires KEDA to be installed in the cluster
# ==============================================================================
apiVersion: keda.sh/v1alpha1
kind: ScaledObject
metadata:
  name: llm-simulator-keda
  namespace: llm-devops
  labels:
    app: llm-simulator
    component: autoscaling
spec:
  scaleTargetRef:
    name: llm-simulator
    kind: Deployment

  minReplicaCount: 1
  maxReplicaCount: 50

  pollingInterval: 15
  cooldownPeriod: 300

  advanced:
    restoreToOriginalReplicaCount: false
    horizontalPodAutoscalerConfig:
      behavior:
        scaleDown:
          stabilizationWindowSeconds: 300
          policies:
            - type: Percent
              value: 50
              periodSeconds: 60
        scaleUp:
          stabilizationWindowSeconds: 30
          policies:
            - type: Percent
              value: 100
              periodSeconds: 15

  triggers:
    # Prometheus-based scaling
    - type: prometheus
      metadata:
        serverAddress: http://prometheus.observability:9090
        metricName: llm_simulator_requests_per_second
        threshold: '100'
        query: |
          sum(rate(llm_simulator_requests_total[1m]))

    # CPU-based scaling
    - type: cpu
      metricType: Utilization
      metadata:
        value: '70'

    # Memory-based scaling
    - type: memory
      metricType: Utilization
      metadata:
        value: '80'

    # Kafka-based scaling (if using message queue)
    # - type: kafka
    #   metadata:
    #     bootstrapServers: kafka.messaging:9092
    #     consumerGroup: llm-simulator-group
    #     topic: simulation-requests
    #     lagThreshold: '50'

    # Redis-based scaling (if using Redis queue)
    # - type: redis
    #   metadata:
    #     address: redis.cache:6379
    #     listName: simulation_queue
    #     listLength: '10'
